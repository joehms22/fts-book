Spell Checking
==============

Introduction
------------

Everyone makes mistakes, and this is especially true of typists, who oftentimes 
transcribe text without being exact, or copy errors exactly in their haste to 
finish a task. Therefore, spell checking is an important factor of any full-text
search system if the end-user is to find information based upon any reasonable 
input. The user shouldn't be expected to account for the errors made by original
authors.

Another common source of spelling errors is introduced through automatically
transcribed documents, especially those that he been processed by OCR systems.

Techniques
----------

There are two major ways to verify spelling.[1]
1. Phonetically
1. N-Gram

_Phonetic_ spell checkers attempt to find the word that will "sound" most like 
the one that is requested, assuming it isn't already in the list of known words.
Such known words are provided in a dictionary file (on most POSIX systems, this 
is usually in /usr/share/dict/words) and will be broken in to phonemes, so words
such as "kraft" will be changed in to "craft".

_N-Gram_ spell checkers work by finding permutations of a string by swapping 
letters, trimming it to different lengths and/or replacing certain letters with 
others, then performs a search on all of those derivations, finding those that 
are closest to the original and likely to be correct using Bayesian arithmetic.
A [fantastic implementation](http://norvig.com/spell-correct.html) and 
discussion on the topic was written by Peter Norvig. This implementation is 
already in Python, so suits our little FTS engine very nicely.

More recent developments in correction technology allow contextual spelling
"correction", some of which was previewed in Google's now forsaken "Wave" (now 
[Apache Wave](http://incubator.apache.org/wave/)). These systems would allow
checking of technically correct, but grammatically improper sentences, like 
those found in the Spell Checker Poem[2]

	A Little Poem Regarding Computer Spell Checkers...
	
	Eye halve a spelling chequer
	It came with my pea sea
	It plainly marques four my revue
	Miss steaks eye kin knot sea.

	Eye strike a key and type a word
	And weight four it two say
	Weather eye am wrong oar write
	It shows me strait a weigh.

	As soon as a mist ache is maid
	It nose bee fore two long
	And eye can put the error rite
	Its rare lea ever wrong.

	Eye have run this poem threw it
	I am shore your pleased two no
	Its letter perfect awl the weigh
	My chequer tolled me sew. 


Our Implementation
------------------

While fantastic open-source spell checkers exist, such as GNU's [aspell](http://aspell.net/)
there are a few reasons _not_ to use them:

* Half the fun is writing one yourself
* Aspell isn't written in native Python, so porting could be an issue
* External libraries add an extra dependency to your project
* Solutions like Norvig's may not work for our type of documents

We'll be mainly working off Norvig's implementation, but making a few tweaks to 
adjust for a few specific technologies that we may be indexing.

Being we can't train the spelling check for a document off itself, and we really 
don't want to train it off other documents of unknown origin, that leaves us a 
few options:

1. Train off a dictionary of known good words, being clearly incomplete for some
contexts.
1. Train off the collection as a whole (but we don't have an index yet to find
likely words!)

For indexing things like scientific documents, training off the collection would
work best, as dictionaries rarely contain things like species names or advanced
topics. Although, this frequency assumes that there isn't one paper that 
misspells Arabidopsis ten-thousand times and messes with your heuristics. Or, 
if you're indexing Twitter, "4ever" becomes a correct spelling (it currently has
[295,000 results on Google)[https://www.google.com/search?q="4ever"%20site:twitter.com] 
throwing off your dictionary again.

We'll use a dictionary then! Except, Norvig's algorithm uses word frequency in 
an original document to determine which is most likely the answer; this won't 
work for us, so we'll also need a dictionary of frequencies of words in the
original document.

There is *one* other thing that should be added to Norvig's solution, the 
ability to add custom formatters, e.g. for Twitter replacements that won't be 
accounted for because they are more than two letters away from the correct
version.

We'll also add a cache for looked up words, so speed will increase, being the 
slowest part is iterating the second time for unknown words, like usernames, 
that will simply be returned, as a username of N characters will have O(N^2)
number of iterations to check the second time around.

The following spellchecker is the new implementation:

	#!/usr/bin/env python
	'''A simple spell checker for Python, modified version of Peter Norvig's
	http://norvig.com/spell-correct.html
	
	Created by: Joseph Lewis <joehms22 [at] gmail [dot] com> 2012-09-19
	'''
	import re
	import collections

	DICTIONARY_FILE = "american-english"

	class Checker:
		known_words = set()
		check_cache = {}
		alphabet = 'abcdefghijklmnopqrstuvwxyz'
	
		def __init__(self):
			''' Sets up the checker. '''
			with open(DICTIONARY_FILE) as fp:
				self.populate_dict(fp)
		
		def populate_dict(self, fp):
			''' Populates the known-words dictionary. '''
		
			# normalize everything in the aspell dict
			doc = fp.read()
			doc = doc.lower()
			doc = re.sub(r'[^\w\s]+', '', doc) 
		
			for word in doc.split():
				self.known_words.add(word)
			
	
		def get_doc_dict(self, document_words_list):
			'''Gets a word->frequency dict for the words in the given doc.'''
			model = collections.defaultdict(lambda: 1)
			for f in document_words_list:
				model[f] += 1
			return model
		
	
		def check_word(self, word, doc_dict={}):
			''' Checks the given word against the dictionary, returns a "corrected" 
			one. '''
			
			word = word.lower()
		
			try:
				return self.check_cache[word]
			except KeyError:
				pass # faster than if in
		
			if word in self.known_words:
				return word
		
			candidates = self.known([word]) or self.known(self.edits1(word)) or self.known_edits2(word) or [word]
			nuword = max(candidates, key=doc_dict.get)
			if len(candidates) == 1:
				self.check_cache[word] = nuword
			return nuword
		
		
		twitterizations = {"":"", "4":"for", "2":"to", "txt":"text"}
	
		def edits1(self, word):
			twitterizations = [word.replace(k,v) for k,v in self.twitterizations.items()]
			splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]
			deletes    = [a + b[1:] for a, b in splits if b]
			transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]
			replaces   = [a + c + b[1:] for a, b in splits for c in self.alphabet if b]
			inserts    = [a + c + b     for a, b in splits for c in self.alphabet]
			return set(deletes + transposes + replaces + inserts + twitterizations)

		def known_edits2(self, word):
			return set(e2 for e1 in self.edits1(word) for e2 in self.edits1(e1) if e2 in self.known_words)

		def known(self, words): 
			return set(w for w in words if w in self.known_words)

Example
-------

Let's work through an example that will show how the spellchecker will work. 

To begin with, we'd start up Python, and create a new spellchecker:

	>>> import spell_checker
	>>> c = spell_checker.Checker()
	
Now, we'll check the word "bat"

	>>> c.check_word("bat")
	'bat'

bat is returned quickly, because it is found in the dictionary, but what happens
if we check for "zat"?

Our check for "twitterizations" returns only "zat":
	
	['zat', 'zat', 'zat', 'zat']
	
Splits splits the word up to all combinations:

	[('', 'zat'), ('z', 'at'), ('za', 't'), ('zat', '')]

Deletions deletes letters:

	['at', 'zt', 'za']

Transposing the letters does so:

	['azt', 'zta']


Replacing letters creates quite a few "real" words:

	['aat', 'bat', 'cat', 'dat', 'eat', 'fat', 'gat', 'hat', 'iat', 'jat', 'kat', 'lat', 'mat', 'nat', 'oat', 'pat', 'qat', 'rat', 'sat', 'tat', 'uat', 'vat', 'wat', 'xat', 'yat', 'zat', 'zat', 'zbt', 'zct', 'zdt', 'zet', 'zft', 'zgt', 'zht', 'zit', 'zjt', 'zkt', 'zlt', 'zmt', 'znt', 'zot', 'zpt', 'zqt', 'zrt', 'zst', 'ztt', 'zut', 'zvt', 'zwt', 'zxt', 'zyt', 'zzt', 'zaa', 'zab', 'zac', 'zad', 'zae', 'zaf', 'zag', 'zah', 'zai', 'zaj', 'zak', 'zal', 'zam', 'zan', 'zao', 'zap', 'zaq', 'zar', 'zas', 'zat', 'zau', 'zav', 'zaw', 'zax', 'zay', 'zaz']

And inserting letters doesn't do much:

	['azat', 'bzat', 'czat', 'dzat', 'ezat', 'fzat', 'gzat', 'hzat', 'izat', 'jzat', 'kzat', 'lzat', 'mzat', 'nzat', 'ozat', 'pzat', 'qzat', 'rzat', 'szat', 'tzat', 'uzat', 'vzat', 'wzat', 'xzat', 'yzat', 'zzat', 'zaat', 'zbat', 'zcat', 'zdat', 'zeat', 'zfat', 'zgat', 'zhat', 'ziat', 'zjat', 'zkat', 'zlat', 'zmat', 'znat', 'zoat', 'zpat', 'zqat', 'zrat', 'zsat', 'ztat', 'zuat', 'zvat', 'zwat', 'zxat', 'zyat', 'zzat', 'zaat', 'zabt', 'zact', 'zadt', 'zaet', 'zaft', 'zagt', 'zaht', 'zait', 'zajt', 'zakt', 'zalt', 'zamt', 'zant', 'zaot', 'zapt', 'zaqt', 'zart', 'zast', 'zatt', 'zaut', 'zavt', 'zawt', 'zaxt', 'zayt', 'zazt', 'zata', 'zatb', 'zatc', 'zatd', 'zate', 'zatf', 'zatg', 'zath', 'zati', 'zatj', 'zatk', 'zatl', 'zatm', 'zatn', 'zato', 'zatp', 'zatq', 'zatr', 'zats', 'zatt', 'zatu', 'zatv', 'zatw', 'zatx', 'zaty', 'zatz']


All of these together produce the set of known words:

	set(['tat', 'bat', 'pat', 'mat', 'zit', 'fat', 'cat', 'rat', 'oat', 'vat', 'nat', 'sat', 'hat', 'eat', 'zap', 'at'])

Then, we return the first found, _or_ that which is most found in the document,
in this case we didn't supply a document frequency map to `check_word` so it 
chooses the first. If we had provided the map:

	{'bat':14, 'plant':450, 'vat':3}

"bat" would have been returned.

Sources
-------

[1] A Comparison of Standard Spell Checking Algorithms and a Novel Binary Neural Approach, Victoria J. Hodge and Jim Austin, IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 15, NO. 5, SEPTEMBER/OCTOBER 2003

[2] Jerrold H. Zar, 1992 [link](http://www.latech.edu/tech/liberal-arts/geography/courses/spellchecker.htm)


