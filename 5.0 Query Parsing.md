Query Parsing
=============

![Hi Randall!](https://raw.github.com/joehms22/fts-book/master/Illustrations/exploits_of_a_mom.png)

[Credit: xkcd](http://xkcd.com/327/)

Introduction
------------
In this section, I'll describe one way to implement the Query Parser,
but first, an aside on common query conventions:

Search has come a long way from pages like these:

![A very bad search interface from Web Of Science](https://raw.github.com/joehms22/fts-book/master/Illustrations/web_of_science.png)

This is much due to a change in the way queries are performed on the
backend. Rather than constructing SQL queries for search *databases*
based upon the given terms as is probably happening here, modern search
*engines* have a different approach that allows special characters to
instruct the search engine what you want.

Note that traditional database lookups wouldn't mean much for the
inverted index backends we've built because there are no fields to do
queries on.

When we perform queries on a full text index system, not only does the
old query mechanism become cumbersome, but it gets to be replaced by
something more natural...simply asking for what you want.

Traditional FTS systems suport the following types of queries:

	+	An and, both preceeding and following words must be in the given
		document.

	-	The following word/phrase cannot be in the document

	"	The phrase within the quotes must be in the document exactly

	key:val	The document must have the metadata given associated with it

In combination these queries allow us to create complex searches that
are really quite natural, say you remembered some words:

	To be or not to be

and that they were in some old English guy's piece of literature:

	English, literature, not a song

The query you'd search would be something like this:

	"To be or not to be" English+literature -song

Of course this would work as well:

	to be or not to be english literature

These queries require constructing a new computer language that looks and 
behaves very much like those already in existance. The formal definition for the
search _language_ that we'll implement is this (if you understand this, go 
ahead and skip the next few sections):

	QUERY	= NOTBOOL (,NOTBOOL)*
	NOTBOOL = (-) ANDBOOL
	ANDBOOL = META+ANDBOOL | META
	META	= WORD:WORD  | STRING
	STRING  = "PHRASE" | WORD
	PHRASE  = WORD(, WORD)*
	WORD	= CHAR (, CHAR)*
	CHAR 	= A-Za-z0-9

Concept
-------

In order to turn the string the user inputs in to something the computer 
understands we'll need a tokenizer and a parser, but it may be simpler to 
explain it using the following, smaller, language:

	EXPRESSION = TERM + TERM | TERM
	TERM = FACTOR * FACTOR | FACTOR
	FACTOR = 0-9(, 0-9)*
	
This language can evaluate simple mathematical expressions like the following:

	3 * 5 + 456

In order to first make sense of how we do something like this, we'll need a way
to break apart what the user has made; in our case, we'll say all inputs must
be separated by one or more spaces, then our `tokenizer` can just call `split()`
to get our list of tokens:

	# Python
	>>> "3 * 5 + 456".split()
	['3', '*', '5', '+', '456']

Next, we'll need some functions, each of which will take this list of tokens,
and return the value of whatever that level was, it is easiest to start at the
lowest part of the "language" to implement it, in this case "FACTOR".

	def factor(tokens):
		return int(tokens.pop(0))
	
In this case, factor just needs to recognize integers and upload them, because
factors must be composed of _at least_ one digit 0-9.

A TERM can be either a FACTOR or a FACTOR * FACTOR, so we'll code it like so:

def term(tokens):
	left_side = factor(tokens)
	if len(tokens) != 0 and tokens[0] == "*":
		tokens.pop(0) # throw away the "*"
		return left_side * factor(tokens)
	else:
		return left_side

`term` is a little more complex, it first passes along the token list to 
`factor`, who will return a number. It will then check if the next token is the
multiplication sign, if so, it'll take that off the list, and return the first
factor multiplied by another one, otherwise it'll just return the factor.

An EXPRESSION is just as easy as term, but with the stars replaced with plus 
signs.

	def expression(tokens):
		left_side = term(tokens)
		if len(tokens) != 0 and tokens[0] == "+":
			tokens.pop(0) # throw away the "+"
			return left_side + term(tokens)
		else:
			return left_side

And just for convenience, we'll add a simple method called `calc` that'll 
split apart a string and return the expression of the tokens retrieved by that:

	def calc(string):
		return expression(string.split())

Then, we can run `calc("3 * 5 + 456")` and get the result: `471`. As you can see
writing the query language described earlier can be a somewhat daunting task.


High Level Implementation
-------------------------

The tokenizer splits apart the query in to terms, and special symbols like 
`+,-,:,"` and parses much in the same way the small calculator language did, but
instead of returning a number, it returns a _list of documents_.

Next, we'll dive through each layer examining what it does, instead of going
through the exact code

`QUERY	= NOTBOOL (,NOTBOOL)*`

This section of code makes sure the Query keeps going until nothing is left
in the input, because there are no special symbols required next to every word
for it to be searched, we can't rely on the program to just know to keep going
until the query is done.

The documents from one NOTBOOL are joined with the documents from another and 
everything is returned.


`NOTBOOL = (-) ANDBOOL`

A NOTBOOL is either just an ANDBOOL or one with a minus sign in front of it.

If there is a minus sign in front of it, the NOTBOOL _removes_ all of the 
documents that come back from evaluating the ANDBOOL from the pre-existing list
of known documents. Thusly, if you put a NOTBOOL in front of any search
it won't get rid of anything because there is nothing to get rid of yet.


`ANDBOOL = META+ANDBOOL | META`

The ANDBOOL is comprised of any large chain of METAS or a single META. 

If the plus sign is used, the ANDBOOL will only return documents that show up in
*both* the first and second part, thus you could search for documents that 
contain both United and States by using "united+states".

`META	= WORD:WORD  | STRING`

A META is a special search that just looks for two words separated by a colon,
if it is found, then the META looks up any documents in the database with 
metadata matching the key and value, and adds those to the list.

It calls `find_documents_for_metadata` in the database class it is given to do
the lookup.

If the user makes a mistake, and enters something like a plus sign after the 
colon, both are discarded and just the word is looked up (we want to be 
forgiving to the user).

`STRING  = "PHRASE" | WORD`

If there are any number of words within quotation marks, the STRING will add the
document numbers retrieved from `documents_with_phrase` to the list of 
documents.

If only one quotation mark is found, another one is assumed before the 
nth - 1 word, where n is the location of the next special symbol.

e.g. In:

	['"','hello','world','author',':','john']

the next special symbol is `:`, thus the next quote is assumed to be after 
`world`.


The PHRASE, WORD, and CHAR portions are handled in the code for STRING, and the
tokenizer.

Actual Implementation
---------------------

The actual implementation is somewhat messy, but can be found in 
`query_parser.py`.
