Normalizing Documents
=====================

One of the largest difficulties modern search engines have is the vast quantity 
of information that they are asked to store and recall upon demand.

A way to cut down on the complexity of storing all the information is to 
"normalize" documents. This generally includes things like stemming the words,
making them lower-case, and stripping all characters that won't be searchable.

Besides shrinking the database, this provides one other major benefit: when a 
person searches for a word, it'll match more variations on that word that are
likely what the user wants.

One thing that many search engines *do* want is the original position of the
words in the text to be kept somewhere. That way, when they need to look for
a specific phrase, or the user wants to reconstruct the document (like when
you open "cached" pages on Google.)

Implementation Overview
-----------------------

Our implementation of normalizing will come in two forms, spell checking and 
stripping. By doing this, `John`, `john`, and `jo-hn` will all be stored in one 
field, `john`. We'll also want to store original spelling distinctly from 
the spell-corrected version, so if our spell-checker isn't correct about a word
the user will still be able to find the document with the right one.

Imagine a hypothetical document:

	I like kats. Cats are cool, because they are fluffy.

With the hypothetical metadata:

	author:Kate Smith
	level:second grade

First we'll want to normalize all the words and metadata by removing 
non-language characters, and lowercasing everything:

Document:

	i like kats cats are cool because they are fluffy
	
Metadata:

	author:kate smith
	level:second grade


Next, we'll want to spellcheck the document, but in order to keep the original
positions of the words, we'll mark all of the spellchecked words as having the 
position in the document of -1.

Document (words before the bar have a negative index):

	cats | i like kats cats are cool because they are fluffy
	
Metadata:

	author:kate smith
	level:second grade

There is one more thing we can do to make the searches for the database easier.
As an example, let's say you were looking for documents by "kate smith", but 
didn't know if they would have an "author", "artist", or "writer" attribute.

Document (words before the bar have a negative index):

	kate smith second grade cats | i like kats cats are cool because they are fluffy
	
Metadata:

	author:kate smith
	level:second grade

When constructing our document, we'll take all of the metadata values, and add
them to the document with an index of -2. That way when we later build the 
querier, we don't have to tell it to search all text and metadata, and when we
build the document re-constructor we can tell it to ignore any positions with
a location less than 0.

Implementation Breakdown
------------------------

Our implementation will be in two different files, one that does generic
parsing of byte streams in to documents, and another that does spell checking.

The following functions/classes can be found in `document.py`.


	def parse_document(doc):
		doc = doc.lower()
		doc = re.sub(r'[^\w\s]+', '', doc)
		return doc.split()

This function takes a huge string (representing the full text of a document), 
converts it to lower case, removes all non-whitespace and non-language 
characters, (punctuation) and splits it in to a list. e.g.
`"Hello, world" -> ['hello','world']`


	def normalize(string):
		'''Normalizes a string, while replacing all whitespace with underscores.'''
		return "_".join(parse_document(string))

This function takes does the same as parse_document, but replaces all spaces 
with underscores, creating a single string e.g.:
`"Hello, world" -> 'hello_world'` while not used here, it is used in the 
database to normalize searches for metadata.


The document class is probably the most important in the whole search engine.
It takes a stream, uri and dictionary of metadata pairs (representig the file 
text the file path, and properties of the text) and parses it in to something
that the database can understand, it is broken down below for clarity:

	class Document:
		def __init__(self, fd, uri, metadatadict={}):
			''' Indexes/normalizes a document.'''
			
			# set all properties on the document.
			self.metadata = metadatadict
			self.uri = uri
			self.words_map = collections.defaultdict(lambda: [])

The above lines begin the constructor, they store the metadata and uri as 
properties of this Document, and create a new `defaultdict`, essentially a map
that gets a default value of an empty list for new items.

			read = fd.read(BYTES_TO_READ).decode("utf-8")

This line reads a number of bytes from the file and decodes it as utf-8. This 
ensures we won't read the whole text of a file (imagine something as long as 
War and Peace!) and supports international characters, so your search engine
should work with documents from all nationalities.

You may want to try adjusting the `BYTES_TO_READ` variable, Google and our 
example engine store the first 100KiB (100,000 bytes).
		
			# Normalize the words.
			full_text_terms = parse_document(read)
		
			# Count the # of times each term appears in the document.
			term_frequency_map = sc.get_doc_dict(full_text_terms)

Next, the document asks the spellchecker for a frequency of terms in the full
text. This will be used to boost misspelled word likelyhoods. In our example 
about cats, because "cats" appears already in the document, it would be chosen 
instead of "mats", which to the computer would seem an equally viable option for
replacing "kats".

		
			for location, term in enumerate(full_text_terms):
				# Add the term location to the map of term to locations
				self.words_map[term].append(location)
			
				# Check the spelling of this term, if new add it with
				# a location of -1
				new_term = sc.check_word(term, term_frequency_map)
				if new_term != term:
					self.words_map[new_term].append(-1)

Afterward, the document iterates over all of the words in the split apart full 
text, adding their locations to their respective terms. The document then
looks at the suggested spellcheck for the word, if it doesn't match the original
then it is added to the -1 position:

Example Input:
	['i', 'like', 'kats', 'cats', 'are', 'cool', 'because', 'they', 'are', 'fluffy']

Example Output:
	{'i':[0],'like':[1],'kats':[2],'cats':[-1,3],...}

Finally, the document fetches all metadata values, joins it together with spaces
and parses that, adding all words to the -2 location, this way you can search
metadata, but it won't be visible to the user:

			# append all metadata as index of -2
			for term in parse_document(" ".join(self.metadata.values())):
				self.words_map[term].append(-2)

The spell checking algorithm can be found in the "2.1 SpellCheck Interlude" 
chapter.
